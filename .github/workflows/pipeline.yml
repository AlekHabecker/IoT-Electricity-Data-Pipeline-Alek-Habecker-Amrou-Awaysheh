# GitHub Actions Workflow: IoT Energy Pipeline
#
# Purpose:
#   - Automate data ingestion from MQTT to Kafka
#   - Collect additional data from Azure IoT Hub
#   - Detect anomalies in time-series data using ARIMA-LSTM
#   - Store results in TimescaleDB
#   - Visualize data using Grafana and PowerBI
#
# Usage:
#   1. Add this file to .github/workflows/pipeline.yml
#   2. Ensure required secrets are set in your repository:
#        - AZURE_IOT_CONNECTION_STRING
#        - GRAFANA_API_URL
#        - GRAFANA_API_KEY
#        - POWERBI_API_TOKEN
#        - POWERBI_DATASET_ID
#   3. Push to the main branch or wait for the hourly schedule

name: IoT Energy Pipeline

on:
  push:
    branches:
      - main
  schedule:
    - cron: '0 */1 * * *'  # Run every hour

jobs:
  data_ingestion:
    runs-on: ubuntu-latest

    services:
      mqtt:
        image: eclipse-mosquitto:2.0
        ports:
          - 1883:1883

      zookeeper:
        image: bitnami/zookeeper:latest
        ports:
          - 2181:2181
        env:
          ALLOW_ANONYMOUS_LOGIN: yes

      kafka:
        image: bitnami/kafka:latest
        ports:
          - 9092:9092
        env:
          KAFKA_BROKER_ID: 1
          KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
          KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
          ALLOW_PLAINTEXT_LISTENER: yes

      timescaledb:
        image: timescale/timescaledb:latest-pg14
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: password
          POSTGRES_DB: energy_data

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Start MQTT to Kafka ingestion
      run: |
        python ingestion/kafka_producer.py --mqtt-host localhost --mqtt-port 1883 --kafka-host localhost:9092
      env:
        MQTT_TOPIC: 'iot/sensors/#'
        KAFKA_TOPIC: 'energy_data'

    - name: Data Collection with Azure IoT Hub
      run: |
        echo "Azure IoT Hub integration step (assumes Azure CLI or SDK usage here)"
      env:
        AZURE_IOT_CONNECTION_STRING: ${{ secrets.AZURE_IOT_CONNECTION_STRING }}

    - name: Run ARIMA-LSTM anomaly detection
      run: |
        python processing/arima_lstm_anomaly.py
      env:
        DATABASE_URL: postgresql://postgres:password@localhost:5432/energy_data

    - name: Push results to TimescaleDB
      run: |
        python utils/db_utils.py --action write
      env:
        DATABASE_URL: postgresql://postgres:password@localhost:5432/energy_data

    - name: Deploy Grafana Dashboard
      run: |
        echo "Deploying to Grafana via API (visual monitoring of TimescaleDB)"
      env:
        GRAFANA_API_URL: ${{ secrets.GRAFANA_API_URL }}
        GRAFANA_API_KEY: ${{ secrets.GRAFANA_API_KEY }}

    - name: Export PowerBI Reports
      run: |
        echo "Exporting processed data to PowerBI (business reporting & analysis)"
      env:
        POWERBI_API_TOKEN: ${{ secrets.POWERBI_API_TOKEN }}
        POWERBI_DATASET_ID: ${{ secrets.POWERBI_DATASET_ID }}

    - name: Sync Grafana and PowerBI
      run: |
        echo "Synchronizing metrics: Grafana for real-time insights, PowerBI for analytics summaries"
